import os
from fastapi import FastAPI
from contextlib import asynccontextmanager
import multiprocessing
from pydantic import BaseModel
import torch
import numpy as np
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from PIL import Image
from celery import Celery
import time
import gc

@asynccontextmanager
async def lifespan(app: FastAPI):
    yield
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    gc.collect()
    print("ğŸ”¹ GPU Memory Cleared on Server Shutdown")

app = FastAPI(lifespan=lifespan)

# âœ… Celery ì„¤ì • (Redisë¥¼ ë©”ì‹œì§€ íë¡œ ì‚¬ìš©)
celery_app = Celery(
    "tasks",
    broker="redis://172.30.0.7:6379",
    backend="redis://172.30.0.7:6379"
)

# âœ… GPU ì„¤ì • (ê° Workerê°€ ê°œë³„ì ìœ¼ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •)
# os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # CUDA ë””ë²„ê¹… ëª¨ë“œ (í•„ìš”ì‹œ ë¹„í™œì„±í™” ê°€ëŠ¥)

# âœ… Workerë³„ ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ê° Workerê°€ í•œ ë²ˆë§Œ ë¡œë“œ)
sam2_model = None

# âœ… ìš”ì²­ ë°ì´í„° êµ¬ì¡° ì •ì˜
class PredictionRequest(BaseModel):
    image_path: str
    points: list
    labels: list

# âœ… Celery Worker ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ëª¨ë¸ ë¡œë“œ
def load_model():
    """ê° Celery Workerì—ì„œ ì‹¤í–‰ë  ë•Œ ëª¨ë¸ì„ ë¡œë“œ"""
    global sam2_model
    if sam2_model is None:
        print("ğŸ”¹ Loading SAM2 Model...")
        sam2_checkpoint = "checkpoints/sam2.1_hiera_large.pt"
        model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
        sam2_model = build_sam2(model_cfg, sam2_checkpoint, device="cuda")

        worker_name = multiprocessing.current_process().name
        worker_pid = os.getpid()
        print(f"âœ… Worker {worker_name} (PID: {worker_pid}) - SAM2 Model Loaded")

    return sam2_model

# âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ (ë™ì  empty_cache ì ìš©)
def clear_gpu_memory(threshold=1):
    """ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬ê°€ threshold GiB ì´ìƒì´ë©´ empty_cache() ì‹¤í–‰"""
    allocated_memory = torch.cuda.memory_allocated() / 1024**3  # GiB ë‹¨ìœ„ ë³€í™˜
    print('allocated_memory: ', allocated_memory)
    if allocated_memory > threshold:
        print(f"ğŸ”¹ Allocated Memory ({allocated_memory:.2f} GiB) exceeds {threshold} GiB. Clearing cache...")
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        gc.collect()

# âœ… Celery Task ì •ì˜ (SAM2 ì˜ˆì¸¡ì„ ë¹„ë™ê¸° ì‹¤í–‰)
@celery_app.task(name="tasks.predict_task", time_limit=300)
def predict_task(image_path, points, labels):
    """ SAM2 ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë¹„ë™ê¸° Celery Task """
    worker_name = multiprocessing.current_process().name
    worker_pid = os.getpid()
    print(f"ğŸ”¹ Task started on Worker: {worker_name} (PID: {worker_pid})")

    global sam2_model
    if sam2_model is None:
        sam2_model = load_model()

    start_time = time.time()
    print(f"ğŸ”¹ {worker_name} (PID: {worker_pid}) - Celery Task Started")

    try:
        image = np.array(Image.open(image_path).convert("RGB"))

        # âœ… Gradient ê³„ì‚° ì™„ì „ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ìµœì í™”)
        with torch.set_grad_enabled(False):
            predictor = SAM2ImagePredictor(sam2_model)
            predictor.set_image(image)

            # ğŸ”¹ labels ì°¨ì› ìˆ˜ì • (N, 1)ë¡œ ë³€í™˜
            points = np.array(points, dtype=np.float32).reshape(-1, 2)
            labels = np.array(labels, dtype=np.int32).reshape(-1)

            print(f"ğŸ”¹ {worker_name} (PID: {worker_pid}) - Running predictor.predict()")
            masks, _, _ = predictor.predict(
                point_coords=points,
                point_labels=labels,
                multimask_output=False
            )

        elapsed_time = time.time() - start_time

        print(f"âœ… {worker_name} (PID: {worker_pid}) - Prediction Completed in {elapsed_time:.2f} sec")

        return {"masks": masks.tolist()}

    except torch.cuda.OutOfMemoryError:
        print(f"âŒ CUDA OOM Error on Worker {worker_name} (PID: {worker_pid})")
        torch.cuda.empty_cache()
        gc.collect()
        return {"error": "CUDA Out of Memory"}

    except Exception as e:
        print(f"âŒ Unexpected Error on Worker {worker_name} (PID: {worker_pid}): {str(e)}")
        return {"error": str(e)}

    finally:
        # âœ… GPU ë©”ëª¨ë¦¬ ë™ì  ì •ë¦¬ ì ìš© (2GiB ì´ìƒ ì‚¬ìš© ì‹œ ìºì‹œ í•´ì œ)
        clear_gpu_memory(threshold=1)
        print(f"ğŸ”¹ {worker_name} (PID: {worker_pid}) - GPU Memory Cleared After Task")


# âœ… FastAPI ì—”ë“œí¬ì¸íŠ¸ (Celery Task ì‹¤í–‰)
@app.post("/predict")
async def predict_mask(request: PredictionRequest):
    """ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ Celery Taskë¡œ ë„˜ê¸°ê³ , task_id ë°˜í™˜ """
    task = celery_app.send_task("tasks.predict_task", args=[request.image_path, request.points, request.labels])
    return {"task_id": task.id}

# âœ… Task ê²°ê³¼ ì¡°íšŒ API
@app.get("/result/{task_id}")
async def get_result(task_id: str):
    """ Celery Task ê²°ê³¼ ì¡°íšŒ """
    task_result = celery_app.AsyncResult(task_id)
    if task_result.ready():
        return {"status": "completed", "result": task_result.result}
    return {"status": "processing"}

# âœ… FastAPI ì‹¤í–‰
if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)  # CUDA ë¬¸ì œ í•´ê²°
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
